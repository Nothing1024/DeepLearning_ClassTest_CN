{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8acb6eb2",
   "metadata": {},
   "source": [
    "# 一步一步建立神经网络\n",
    "\n",
    "欢迎来到第四周第一部分的练习！之前你已经训练了单隐层的双层神经网络\n",
    "\n",
    "这周你将会建立一个多层神经网络(层数可自定义)\n",
    "\n",
    "- 在本次课程中，你将会实现了建立神经网络需要的所有函数\n",
    "- 在下一个作业中，你将用这些函数来建立一个图像分类的深度神经网络\n",
    "\n",
    "\n",
    "**完成这次任务后，你会学得**\n",
    "\n",
    "- 使用想ReLU这样的非线性单元来提高你的模型\n",
    "- 建立一个深度神经网络 (超过1个隐含层)\n",
    "- 实现一个易用的神经网络类\n",
    "\n",
    "**注释**:\n",
    "- 上标$[l]$表示和第$l^{th}$层相关的量\n",
    "    - 例: $a^{[L]}$ 是第 $L^{th}$层的激活值 $W^{[L]}$和$b^{[L]}$是第$L^{th}$层的参数\n",
    "- 上标$(i)$表示和第$i^{th}$个样本相关的量\n",
    "    - 例: $x^{(i)}$是第$i^{th}$训练样本\n",
    "- 下标$i$表示第 $i^{th}$个向量的输入\n",
    "    - 例: $a^{[l]}_i$ 是第$l^{th}$层第$i^{th}$的激活值\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf39811",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [1 - 导包](#1)\n",
    "- [2 - 概要](#2)\n",
    "- [3 - 初始化](#3)\n",
    "  - [3.1 - 2层神经网络](#3-1)\n",
    "    - [练习 1 - 初始化参数](#ex-1)\n",
    "  - [3.2 - L层神经网络](#3-2)\n",
    "    - [练习 2 - 初始化参数深度](#ex-2)\n",
    "- [4 - 前向传播模块](#4)\n",
    "  - [4.1 - 前向传播-线性部分](#4-1)\n",
    "    - [练习 3 - 前向传播-线性部分](#ex-3)\n",
    "  - [4.2 - 前向传播-激活部分](#4-2)\n",
    "    - [练习 4 - 前向传播-线性+激活部分](#ex-4)\n",
    "  - [4.3 - L层模型](#4-3)\n",
    "    - [练习 5 - L层模型的前向传播](#ex-5)\n",
    "- [5 - 代价函数](#5)\n",
    "  - [练习 6 - 计算cost](#ex-6)\n",
    "- [6 - 后向传播模块](#6)\n",
    "  - [6.1 - 线性后向传播](#6-1)\n",
    "    - [练习 7 - 线性后向传播](#ex-7)\n",
    "  - [6.2 - 后向传播的线性激活](#6-2)\n",
    "    - [练习 8 - 后向传播的线性激活](#ex-8)\n",
    "  - [6.3 - L层模型的后向传播](#6-3)\n",
    "    - [练习 9 - L层模型的后向传播](#ex-9)\n",
    "  - [6.4 - 更新参数](#6-4)\n",
    "    - [练习 10 - 更新参数](#ex-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963d7ca",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - 导包\n",
    "\n",
    "首先你需要导入任务中要用到的相关包\n",
    "\n",
    "- [numpy](https://numpy.org/doc/1.20/):Python中用于科学计算的基础包\n",
    "- [matplotlib](http://matplotlib.org)：Python中著名画图库\n",
    "- dnn_utils 在这次任务中提供各种能帮到你的函数\n",
    "- testCases 提供一些测试案例来评估你函数的正确性\n",
    "- np.random.seed(1) 设置随机数seed，固定结果(方便测试)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbd0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # 设定绘图大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc04b1",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - 概要\n",
    "\n",
    "为了搭建神经网络，你需要先实现几个辅助函数，这些辅助函数将会在后期搭建两层神经网络和L层神经网络用到。每个辅助函数都有详细的说明，指导你完成必要的步骤\n",
    "\n",
    "以下是作业步骤的概要：\n",
    "\n",
    "- 初始化二层(或L层)神经网络要用到的参数\n",
    "- 实现前向传播模块(下图中的紫色部分)\n",
    "     - 完成层中前向传播的线性部分(算出resulting in $Z^{[l]}$)\n",
    "     - 激活函数已经提供(relu/sigmoid)\n",
    "     - 将前两步结合成一个新的[LINEAR->ACTIVATION]前向函数\n",
    "     - 把[LINEAR->RELU]前向叠加(从第1层到第L-1层)，然后在最后添加[LINEAR->SIGMOID]前向函数(第L层)，这时你会得到一个新的L层模型前向函数.\n",
    "- 计算loss\n",
    "- 实现后向传播模块(下图中的红色部分)\n",
    "    - 完成层中后向传播的线性部分\n",
    "    - 激活函数的梯度已经提供(relu_backward/sigmoid_backward) \n",
    "    - 将前两步结合成一个新的[LINEAR->ACTIVATION]后向函数\n",
    "    - 把[LINEAR->RELU]后向叠加L-1次，最后后向叠加一次[LINEAR->SIGMOID]函数，这时你会得到一个新的L层模型后向函数\n",
    "- 最终更新参数\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center><b>图 1</b></center></caption><br>\n",
    "\n",
    "\n",
    "**笔记**:\n",
    "\n",
    "对于每一个前向传播函数，都会有与之匹配的后向函数。这就是为什么你需要在前向模块中的每一步都要存储缓存cache，缓存值在后期计算梯度中很有用\n",
    "\n",
    "在后向传播模块中，你可以用刚才得到的cache计算梯度值。相关步骤本次作业都会带你一步一步地完成，不用担心"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056469bf",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - 初始化\n",
    "\n",
    "你将会编写两个辅助函数，这两个函数用来初始化模型中需要的参数\n",
    "- 第一个函数用来初始化二层模型的参数\n",
    "- 第二个函数用来归纳初始化过程(将参数整合起来)\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - 2层神经网络\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### 练习 1 - 初始化参数\n",
    "\n",
    "创建和初始化2层神经网络参数\n",
    "\n",
    "**说明**:\n",
    "\n",
    "- 模型的整体结构是: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- 对权重矩阵进行随机初始化: `np.random.randn(shape)*0.01`，注意shape不要出错\n",
    "- 对偏移向量进行0初始化: `np.zeros(shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5168e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 初始化参数\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- 输入层大小\n",
    "    n_h -- 隐藏层大小\n",
    "    n_y -- 输出层大小\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python字典，包含以下参数:\n",
    "                    W1 -- weight，权重矩阵，shape为(n_h, n_x)\n",
    "                    b1 -- bias，偏移向量，shape为(n_h, 1)\n",
    "                    W2 -- weight，权重矩阵，shape为(n_y, n_h)\n",
    "                    b2 -- bias，偏移向量，shape为(n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #(预计 4 行代码完成)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5d25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "initialize_parameters_test(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95c304",
   "metadata": {},
   "source": [
    "***期望输出***\n",
    "```\n",
    "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]]\n",
    "b1 = [[0.]\n",
    " [0.]]\n",
    "W2 = [[ 0.01744812 -0.00761207]]\n",
    "b2 = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22369937",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - L层神经网络\n",
    "\n",
    "L层神经网络的初始化更复杂，因为里面会有更多的权重矩阵和偏移向量\n",
    "\n",
    "当实现`initialize_parameters_deep()`函数时，你应该确信你的维度要和每一层相匹配\n",
    "\n",
    "回忆：$n^{[l]}$是第$l$层的单元数. 例如， 如果你的输入数据$X$维度是$(12288, 209)$ (with $m=209$) ，那么：\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> <b>W的shape</b> </td> \n",
    "        <td> <b>b的shape</b>  </td> \n",
    "        <td> <b>激活函数</b> </td>\n",
    "        <td> <b>激活值的shape</b> </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>第1层</b> </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>第2层</b> </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>  \n",
    "   <tr>\n",
    "       <td> <b>第L-1层</b> </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "   <tr>\n",
    "   <tr>\n",
    "       <td> <b>第L层</b> </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "请记住，当你在python计算$W X + b$时会触发广播机制(boradcasting)\n",
    "\n",
    "例：如果: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    w_{00}  & w_{01} & w_{02} \\\\\n",
    "    w_{10}  & w_{11} & w_{12} \\\\\n",
    "    w_{20}  & w_{21} & w_{22} \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    x_{00}  & x_{01} & x_{02} \\\\\n",
    "    x_{10}  & x_{11} & x_{12} \\\\\n",
    "    x_{20}  & x_{21} & x_{22} \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    b_0  \\\\\n",
    "    b_1  \\\\\n",
    "    b_2\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "那么$WX + b$会变成:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n",
    "    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n",
    "    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n",
    "\\end{bmatrix}\\tag{3}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19454f47",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### 练习 2 -  初始化参数深度\n",
    "\n",
    "实现一个L层神经网络的初始化\n",
    "\n",
    "**说明**:\n",
    "- 模型的结构是 *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., 其中$L-1$层使用ReLU激活函数，最后的输出层使用sigmoid激活函数\n",
    "- 对权重矩阵使用随机初始化(`np.random.randn(shape) * 0.01`)\n",
    "- 对偏移向量使用0初始化(`np.zeros(shape)`)\n",
    "- 你将会用变量`layer_dims`来存储不同层对应的单元数$n^{[l]}$，例如上周的二维数据分类模型对应的是 [2,4,1] (输入层有2个单元，有1个隐含层且隐含层有4个隐含单元，输出层有1个单元)，这也意味着`W1`的shape是(4,2)，`b1`的shape是(4,1)，`W2`的shape是(1,4)，`b2`的shape是(1,1)。现在你将会把这个概念推广到L层\n",
    "- 这里是L=1，也就是1层神经网络的实现，应该会对你对通用情况(L层神经网络)下的实现有启发\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7106e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 初始化参数深度\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python的array (list)它包含我们神经网络中每一层的维度\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python的dictionary，它包含参数\"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- 权重矩阵，shape为(layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- 偏移向量，shape为 (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # 网络层数\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #(预计 2 行代码完成)\n",
    "        # parameters['W' + str(l)] = ...\n",
    "        # parameters['b' + str(l)] = ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d83570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "initialize_parameters_deep_test(initialize_parameters_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ceffd",
   "metadata": {},
   "source": [
    "***期望输出***\n",
    "```\n",
    "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21197367",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - 前向传播模块\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - 前向传播-线性部分\n",
    "\n",
    "完成了参数初始化后你就可以继续完成前向传播模块了\n",
    "\n",
    "首先我们先实现一些必要的辅助函数，用于后期模型的实现，现在将要按次序完成以下函数：\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION (ACTIVATION是ReLU或Sigmoid). \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (整个模型)\n",
    "\n",
    "线性前向传播模块(所有样本向量化)计算公式如下：\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "其中 $A^{[0]} = X$. \n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### 练习 3 - 前向传播-线性部分\n",
    "\n",
    "建立前向传播的线性部分\n",
    "\n",
    "\n",
    "**提醒**:\n",
    "数学公式$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$的实现可以借助 `np.dot()`函数.，记住用`W.shape`来检查维度是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35b49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 前向传播-线性部分\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    实现前向传播的线性部分\n",
    "\n",
    "    Arguments:\n",
    "    A -- 前一层(或输出层)的激活值: (上一层单元数, 样本数)\n",
    "    W -- 权重矩阵: numpy array，shape为(当前层单元数, 上一层单元数)\n",
    "    b -- 偏移向量, numpy array，shape为(当前层单元数, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- 激活函数的输入值，也叫做预激活参数 \n",
    "    cache -- python tuple，包含\"A\", \"W\" and \"b\" ; 存储后向传播要用到的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    #(预计 1 行代码完成)\n",
    "    # Z = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9a6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_A, t_W, t_b = linear_forward_test_case()\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "print(\"Z = \" + str(t_Z))\n",
    "\n",
    "linear_forward_test(linear_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7d951",
   "metadata": {},
   "source": [
    "***期望输出***\n",
    "```\n",
    "Z = [[ 3.26295337 -1.23429987]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420689c",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - 前向传播-激活部分\n",
    "\n",
    "在本次练习中，你将会用到两个激活函数：\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}} \\ \\$ 现在`sigmoid`函数已经给出，该函数有两个返回值：激活值`a`和`cache`(包含后向传播会用到的`z`)，以下是它的调用方法：\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: 数学定义为$A = RELU(Z) = max(0, Z)$. 现在`relu`函数已经给出，该函数有两个返回值：激活值`a`和`cache`(包含后向传播会用到的`z`)，以下是它的调用方法：\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7acc69",
   "metadata": {},
   "source": [
    "为了方便，你可以把两个函数(Linear and Activation)合并成一个函数(LINEAR->ACTIVATION)\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### 练习 4 - 前向传播-线性+激活部分\n",
    "\n",
    "该部分主要实现前向传播的 *LINEAR->ACTIVATION* 层，数学关联为$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$\n",
    "\n",
    "其中激活部分既可以是`sigmoid()`函数也可以是`relu()`函数. \n",
    "\n",
    "该函数需要借助`linear_forward()`函数和正确的激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5580c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 前向传播-线性+激活部分\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    实现前向传播的LINEAR->ACTIVATION层\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- 上一层的激活值(或者是输入函数)(上一层单元数, 样本数)\n",
    "    W -- 权重矩阵: numpy array，shape为(当前层单元数, 上一层单元数)\n",
    "    b -- 偏移向量, numpy array，shape为(当前层单元数, 1)\n",
    "    activation -- 这层要用到的激活函数 有两种string模式: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- 激活函数输出值，也被叫做激活后的值\n",
    "    cache -- 一个python的tuple, 包含\"linear_cache\" and \"activation_cache\"，\n",
    "             后向传播计算时会用到\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        #(预计 2 行代码完成)\n",
    "        # Z, linear_cache = ...\n",
    "        # A, activation_cache = ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        #(预计 2 行代码完成)\n",
    "        # Z, linear_cache = ...\n",
    "        # A, activation_cache = ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca898e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_A_prev, t_W, t_b = linear_activation_forward_test_case()\n",
    "\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(t_A))\n",
    "\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(t_A))\n",
    "\n",
    "linear_activation_forward_test(linear_activation_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248f6c4",
   "metadata": {},
   "source": [
    "***期望输出***\n",
    "```\n",
    "With sigmoid: A = [[0.96890023 0.11013289]]\n",
    "With ReLU: A = [[3.43896131 0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be828e1e",
   "metadata": {},
   "source": [
    "**注意**: 在深度学习中，\"[LINEAR->ACTIVATION]\"在神经网络中算一层，不是两层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fdc69",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - L层模型\n",
    "\n",
    "为了在实现L层神经网络时更加方便，你将需要一个函数，这个函数需要复制RELU类型的`linear_activation_forward()`函数$L-1$次，然后使用一个SIGMOID类型的`linear_activation_forward()`函数\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> <b>Figure 2</b> : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* 模型</center></caption><br>\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### 练习 5 -  L层模型的前向传播\n",
    "\n",
    "实现上图模型的前向传播\n",
    "\n",
    "**说明**: 在下行代码中，变量`AL`代表$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$ （有时候也会被叫做`Y`帽$\\hat{Y}$）\n",
    "\n",
    "**提示**:\n",
    "- 使用你刚才编写的函数\n",
    "- 使用for循环[LINEAR->RELU]复制，(L-1)次\n",
    "- 不要忘记追踪`cache`列表中的缓存，你可以使用`list.append(c)`把 `c`尾插到`list`里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84bfec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: L层模型的前向传播\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    实现前向传播计算：[LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 输入数据，numpy数组，shape为(输入层单元数, 样本数)\n",
    "    parameters -- initialize_parameters_deep()函数的输出\n",
    "    \n",
    "    Returns:\n",
    "    AL -- 输出层(也就是最后一层)的激活值\n",
    "    caches -- 缓存列表，包含linear_activation_forward()函数中的缓存(用分L层，索引值从0到L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  #神经网络层数 3层\n",
    "#     print(parameters['W1'].shape,parameters['W2'].shape,parameters['W3'].shape,parameters['b1'].shape,parameters['b2'].shape,parameters['b3'].shape)\n",
    "#     测试案例对应的：(4, 5) (3, 4) (1, 3) (4, 1) (3, 1) (1, 1)\n",
    "    # 实现[LINEAR -> RELU]*(L-1). 把\"cache\"添加到\"caches\"列表\n",
    "    # 从层1开始循环，因为层0是输入层\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        #(预计 2 行代码完成)\n",
    "        # A, cache = ...\n",
    "        # caches ...\n",
    "        # 代码练习区-起始部分\n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "    \n",
    "    # 实现LINEAR -> SIGMOID. 把\"cache\"添加到\"caches\"列表\n",
    "    #(预计 2 行代码完成)\n",
    "    # AL, cache = ...\n",
    "    # caches ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04970b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_X, t_parameters = L_model_forward_test_case_2hidden()\n",
    "t_AL, t_caches = L_model_forward(t_X, t_parameters)\n",
    "\n",
    "print(\"AL = \" + str(t_AL))\n",
    "\n",
    "L_model_forward_test(L_model_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b568853",
   "metadata": {},
   "source": [
    "***期望输出***\n",
    "```\n",
    "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c917461",
   "metadata": {},
   "source": [
    "**真棒!** 你已经实现了一个完整的前向传播，接受输入X输出包含预测的$A^{[L]}$行向量。它将所有的中间值记录到`caches`，你可以用Using $A^{[L]}$计算你预测的cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6989ad",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - 代价函数\n",
    "\n",
    "现在你可以实现前向和后向传播了！为了检查你的模型是否真正在学习，你需要计算cost\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### 练习 6 - 计算cost\n",
    "\n",
    "使用公式$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$ 计算交叉熵cost$J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda9ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    根据(7)式实现代价函数\n",
    "\n",
    "    Arguments:\n",
    "    AL -- 预测结果向量(原：probability vector corresponding to your label predictions), shape为(1, 样本数)\n",
    "    Y -- 实际结果向量 (猫1非猫0), shape为(1, 样本数)\n",
    "\n",
    "    Returns:\n",
    "    cost -- 交叉熵cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # 根据aL和y计算cost\n",
    "    # (预计 1 行代码完成)\n",
    "    # cost = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    \n",
    "    cost = np.squeeze(cost)      # 注意shape要正确 (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "812a32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.2797765635793422\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_Y, t_AL = compute_cost_test_case()\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "\n",
    "print(\"Cost: \" + str(t_cost))\n",
    "\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4491f36",
   "metadata": {},
   "source": [
    "**期望输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>cost</b> </td>\n",
    "    <td> 0.2797765635793422</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e32c0",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - 后向传播模块\n",
    "\n",
    "像前向传播一样，在实现后向传播前你需要实现集合辅助函数，请记住后向传播是用来计算损失函数相对于参数的梯度\n",
    "\n",
    "**暗示**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center><font color='purple'><b>图 3</b>: LINEAR->RELU->LINEAR->SIGMOID 的前向传播(紫色)与后向传播(红色) </font></center></caption>\n",
    "\n",
    "\n",
    "**解释(MD注释部分)**\n",
    "\n",
    "对于你们这样的计算专家(你不需要完成这项任务)，链式求导法则可以被用来计算$\\mathcal{L}$对$z^{[1]}$的导数，比如2层网络的计算就是 the chain rule of calculus can be used to derive:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "为了计算$dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$,你可以根据链式求导法则得到$dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. \n",
    "在后向传播时， 在每一步，你用你当前的梯度乘以特定层所对应的梯度，得到你想要的梯度。(原文：During backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.)\n",
    "\n",
    "为了计算梯度$db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, 你可以根据链式求导法则得到$db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "这就是为什么我们要讨论 **反向传播**.\n",
    "\n",
    "--\n",
    "\n",
    "现在和前向传播相似，你要通过以下三步实现反向传播：\n",
    "\n",
    "1. LINEAR backward\n",
    "2. LINEAR -> ACTIVATION backward (其中ACTIVATION是ReLU或sigmoid激活函数)\n",
    "3. [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (整个模型)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a360928",
   "metadata": {},
   "source": [
    "为了下个练习，你需要记住:\n",
    "\n",
    "- `b`是一个n行1列的矩阵(np.ndarray), i.e: b = [[1.0], [2.0]] (请记住`b`是一个常量)\n",
    "- np.sum对数组的元素进行求和\n",
    "- axis=1表示按行求和(跨列)，axis=0表示按列求和(跨行)\n",
    "- keepdims指定是否必须保留矩阵的原始尺寸\n",
    "- 下例可以帮助你理解内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ac05d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis=1 and keepdims=True\n",
      "[[3]\n",
      " [7]]\n",
      "axis=1 and keepdims=False\n",
      "[3 7]\n",
      "axis=0 and keepdims=True\n",
      "[[4 6]]\n",
      "axis=0 and keepdims=False\n",
      "[4 6]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "print('axis=1 and keepdims=True')\n",
    "print(np.sum(A, axis=1, keepdims=True))\n",
    "print('axis=1 and keepdims=False')\n",
    "print(np.sum(A, axis=1, keepdims=False))\n",
    "print('axis=0 and keepdims=True')\n",
    "print(np.sum(A, axis=0, keepdims=True))\n",
    "print('axis=0 and keepdims=False')\n",
    "print(np.sum(A, axis=0, keepdims=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acf7c1",
   "metadata": {},
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - 线性后向传播\n",
    "\n",
    "$l$层的线性部分为: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (然后在激活).\n",
    "\n",
    "假定你已经计算了导数 $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. 你需要得到 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>图 4</b></font></center></caption>\n",
    "\n",
    "用$dZ^{[l]}$计算输出$(dW^{[l]}, db^{[l]}, dA^{[l-1]})$三个变量\n",
    "\n",
    "\n",
    "附上相关公式:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "\n",
    "$A^{[l-1] T}$是$A^{[l-1]}$的转置. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f3884",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### 练习 7 - 线性后向传播\n",
    "\n",
    "用上面三个公式是实现`linear_backward()`函数\n",
    "\n",
    "**提示**:\n",
    "\n",
    "- 你可以使用`A.T`或`A.transpose()`实现`A`的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4354b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 线性后向传播\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    实现第l层后向传播的线性部分\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- l层成本相对于线性输出的梯度\n",
    "    cache -- python的tuple(A_prev, W, b)在当前层的后向传播\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- 相对于激活值的成本梯度 (l-1层), 和A_prev的shape相同\n",
    "    dW -- 相对于W的成本梯度 (l层), 和W的shape相同\n",
    "    db -- 相对于b的成本梯度 (l层), 和b的shape相同\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### 代码区 ### (预计 3 行代码完成)\n",
    "    # dW = ...\n",
    "    # db = ... 按dz行求和,keepdims=True\n",
    "    # dA_prev = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8050d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db: [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_dZ, t_linear_cache = linear_backward_test_case()\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "\n",
    "print(\"dA_prev: \" + str(t_dA_prev))\n",
    "print(\"dW: \" + str(t_dW))\n",
    "print(\"db: \" + str(t_db))\n",
    "\n",
    "linear_backward_test(linear_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f2f52",
   "metadata": {},
   "source": [
    "**期望输出**:\n",
    "```\n",
    "dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "db: [[-0.14713786]\n",
    " [-0.11313155]\n",
    " [-0.13209101]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cb2e5",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - 后向传播的线性激活\n",
    "\n",
    "之后你将会创建一个函数，将两个辅助函数`linear_backward()`函数和`linear_activation_backward()`函数\n",
    "\n",
    "为了帮助您实现`linear_activation_backward()`函数，现在已经提供了两个函数\n",
    "\n",
    "- **`sigmoid_backward`**: 实现SIGMOID单元的后向传播过程，调用方式如下：\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: 实现RELU单元的后向传播过程，调用方式如下：\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "如果$g(.)$是激活函数，使用`sigmoid_backward`和`relu_backward`计算的方法如下：$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{11}$$  \n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### 练习 8 -  后向传播的线性激活\n",
    "\n",
    "实现*LINEAR->ACTIVATION*层的后向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23d65e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 后向传播的线性激活\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    实现LINEAR->ACTIVATION层的后向传播\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- 当前l层的激活梯度\n",
    "    cache -- tuple，值为(linear_cache, activation_cache)，用于后向传播\n",
    "    activation -- 该层的激活函数：\"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- 相对于激活值的成本梯度 (l-1层), 和A_prev的shape相同\n",
    "    dW -- 相对于W的成本梯度 (l层), 和W的shape相同\n",
    "    db -- 相对于b的成本梯度 (l层), 和b的shape相同\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        #(预计 2 行代码完成)\n",
    "        # dZ =  ...\n",
    "        # dA_prev, dW, db =  ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        #(预计 2 行代码完成)\n",
    "        # dZ =  ...\n",
    "        # dA_prev, dW, db =  ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d0f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "With sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "With sigmoid: db = [[-0.05729622]]\n",
      "With relu: dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "With relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "With relu: db = [[-0.20837892]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: dA_prev = \" + str(t_dA_prev))\n",
    "print(\"With sigmoid: dW = \" + str(t_dW))\n",
    "print(\"With sigmoid: db = \" + str(t_db))\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "print(\"With relu: dA_prev = \" + str(t_dA_prev))\n",
    "print(\"With relu: dW = \" + str(t_dW))\n",
    "print(\"With relu: db = \" + str(t_db))\n",
    "\n",
    "linear_activation_backward_test(linear_activation_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f15dd1",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>\n",
    "### 6.3 - L层模型的后向传播\n",
    "\n",
    "现在你将要实现整个网络的后向传播函数\n",
    "\n",
    "回忆以下你实现`L_model_forward()`函数的适合，每一次迭代你都会有一个缓存(X,W,b,和 z)\n",
    "\n",
    "在后向传播模块，你将要使用这些变量去计算梯度，因此在`L_model_backward()`函数，你将会从$L$层向后遍历所有的隐藏层，每一步你都会使用$l$层的缓存值进行$l$层后向传播\n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>图 5</b>: 后向传播的过程</font></center></caption>\n",
    "\n",
    "**后向传播的初始化**:\n",
    "\n",
    "为了让整个网络进行后向传播，你知道输出值$A^{[L]} = \\sigma(Z^{[L]})$，所以你需要去计算 `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$，代码实现如下：\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # cost对于AL的导数\n",
    "```\n",
    "\n",
    "然后你就可以使用激活后梯度`dAL`来进行后向传播，在图五，你现在可以将`dAL`用到你要实现的LINEAR->SIGMOID后向传播部分(该过程将使用`L_model_forward()`函数进行缓存存储)\n",
    "\n",
    "在那之后，你将会用到for循环对其他的层进行LINEAR->RELU后向传播操作，你应该在梯度字典里存储每层的dA, dW和db。以下是可能会用到的公式\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "例如，当$l=3$时，应该将$dW^{[l]}$存储至`grads[\"dW3\"]`.\n",
    "\n",
    "<a name='ex-9'></a>\n",
    "### 练习 9 -  L层模型的后向传播\n",
    "\n",
    "实现*[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*模型的后向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b004a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: L层模型的后向传播\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    实现*[LINEAR->RELU]  ×  (L-1) -> LINEAR -> SIGMOID*模型的后向传播\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- 可能性向量, 前向传播的输出 (L_model_forward())\n",
    "    Y -- 实际值向量 (猫1非猫0)\n",
    "    caches -- caches列表，包含:\n",
    "                linear_activation_forward()每一个cache\n",
    "                其中：relu：it's caches[l], for l in range(L-1) i.e l = 0...L-2\n",
    "                      sigmoid：it's caches[L-1]\n",
    "                      \n",
    "    Returns:\n",
    "    grads -- python字典，包含梯度\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # 神经网络的层数\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # 将Y的shape和AL的shape一致\n",
    "    \n",
    "    # 初始化后向传播\n",
    "    #(预计 1 行代码完成)\n",
    "    # dAL = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(预计 5 行代码完成)\n",
    "    # current_cache = ...\n",
    "    # dA_prev_temp, dW_temp, db_temp = ...\n",
    "    # grads[\"dA\" + str(L-1)] = ...\n",
    "    # grads[\"dW\" + str(L)] = ...\n",
    "    # grads[\"db\" + str(L)] = ...\n",
    "    # 代码练习区-起始部分\n",
    "    \n",
    "\n",
    "    \n",
    "    # 代码练习区-结束部分\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        #(预计 5 行代码完成)\n",
    "        # current_cache = ...\n",
    "        # dA_prev_temp, dW_temp, db_temp = ...\n",
    "        # grads[\"dA\" + str(l)] = ...\n",
    "        # grads[\"dW\" + str(l + 1)] = ...\n",
    "        # grads[\"db\" + str(l + 1)] = ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb2fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA0 = [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n",
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "db2 = [[0.15187861]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_AL, t_Y_assess, t_caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "print(\"dA0 = \" + str(grads['dA0']))\n",
    "print(\"dA1 = \" + str(grads['dA1']))\n",
    "print(\"dW1 = \" + str(grads['dW1']))\n",
    "print(\"dW2 = \" + str(grads['dW2']))\n",
    "print(\"db1 = \" + str(grads['db1']))\n",
    "print(\"db2 = \" + str(grads['db2']))\n",
    "\n",
    "L_model_backward_test(L_model_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb69286",
   "metadata": {},
   "source": [
    "**期望输出：**\n",
    "\n",
    "```\n",
    "dA0 = [[ 0.          0.52257901]\n",
    " [ 0.         -0.3269206 ]\n",
    " [ 0.         -0.32070404]\n",
    " [ 0.         -0.74079187]]\n",
    "dA1 = [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]]\n",
    "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
    " [0.         0.         0.         0.        ]\n",
    " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
    "dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n",
    "db1 = [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]]\n",
    "db2 = [[0.15187861]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b110df7",
   "metadata": {},
   "source": [
    "<a name='6-4'></a>\n",
    "### 6.4 - 更新参数\n",
    "\n",
    "在这节，你将会用梯度下降更新模型参数：\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "其中 $\\alpha$是学习率 \n",
    "\n",
    "更新参数后记得将他们存储到`parameters()`字典里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90ea01",
   "metadata": {},
   "source": [
    "<a name='ex-10'></a>\n",
    "### 练习 10 - 更新参数\n",
    "\n",
    "完成`update_parameters()`，通过梯度下降更新你的参数\n",
    "\n",
    "**提示**:\n",
    "要更新每一个$W^{[l]}$和$b^{[l]}$ ($l = 1, 2, ..., L$)的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e569c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级函数: 更新参数\n",
    "\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    通过梯度下降更新参数\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python字典，包含你的参数\n",
    "    grads -- python字典，包含你的梯度，即L_model_backward的输出\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python字典，包含你更新的参数\n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # 神经网络的层数\n",
    "\n",
    "    # 使用循环更新每个参数\n",
    "    #(预计 2 行代码完成)\n",
    "    for l in range(L):\n",
    "        # parameters[\"W\" + str(l+1)] = ...\n",
    "        # parameters[\"b\" + str(l+1)] = ...\n",
    "        # 代码练习区-起始部分\n",
    "        \n",
    "\n",
    "        \n",
    "        # 代码练习区-结束部分\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "513c66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n",
      "\u001b[92m 测试全部通过\n"
     ]
    }
   ],
   "source": [
    "t_parameters, grads = update_parameters_test_case()\n",
    "t_parameters = update_parameters(t_parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(t_parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(t_parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(t_parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(t_parameters[\"b2\"]))\n",
    "\n",
    "update_parameters_test(update_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662397cc",
   "metadata": {},
   "source": [
    "**期望输出：**\n",
    "\n",
    "```\n",
    "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
    "b1 = [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]]\n",
    "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
    "b2 = [[-0.84610769]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7dd45",
   "metadata": {},
   "source": [
    "### 恭喜你! \n",
    "\n",
    "你刚才完成了所有建立深度神经网络用到的函数，其中包括：\n",
    "\n",
    "- 使用非线性单元改善模型\n",
    "- 建立一个多层神经网络(隐含层数大于1)\n",
    "- 实现一个易用的神经网络类\n",
    "\n",
    "这次的任务确实有些困难，但下任务会简单很多;) \n",
    "\n",
    "在下个任务中，你将会把这些函数合并成两个模型\n",
    "\n",
    "- 2层神经网络\n",
    "- L层神经网络\n",
    "\n",
    "事实上，你可以使用这些模型来识别图像是否为猫！做的不错，下次我们再见！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}